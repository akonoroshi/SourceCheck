# Hint Rating Data

This folder contains

the QualityScore procedure defined in [1]

## Use Cases

## Datasets

There are currently 3 datasets, collected from two learning environments for introductory programming that offer data-driven hints: iSnap [2], a block-based programming environment and ITAP [3], an intelligent tutoring system (ITS) for Python programming. Both datasets consist of log data collected from students working on multiple programming problems, including complete traces of their code and records of hints they requested.   Both datasets are available from the PSLC Datashop (pslcdatashop.org) \cite{Koedinger2010}.

* **isnapF16-F17**: This  dataset was collected from an introductory programming course for non-majors during the Fall 2016, Spring 2017 and Fall 2017 semesters, with 171 total students completing 6 programming problems, or which only 2 are included in this dataset.
* **isnapF16-S17**: A subset of the isnapF16-F17 dataset that only includes the Fall 2016 and Spring 2017 semesters. This dataset was used in [1]. It consists of data from 120 total students.
* **itapF16-F17**: This dataset was collected from two introductory programming courses in Spring 2016, with 89 total students completing up to 40 Python problems, of which only 5 are included in this dataset (see [4] for details).

Each dataset contains the follow files, which are explained in the following sections:

* gold-standard.csv
* training-data.csv
* [snap | python]-grammar.json

## Training and Request Data

## Gold Standard Data

The gold-standard.csv file contains the human-tutor-authored gold standard hints, which are used to evaluate the quality of other, automatically-generated hints. The hints were generated in three phases (see [1] for details):
1. Three tutors independently reviewed each hint request in the dataset, including the history of the student's code before the hint request. Each tutor then generated a set of all hints they considered to be valid, useful, and not confusing. Each hint was represented as one or more edits to the student's code, making these hints comparable to the edit-based hints offered by many data-driven algorithms.  Tutors were instructed to limit their hints to one edit (e.g. one insertion) unless multiple edits were needed to avoid confusion. Hints were designed to be independently useful, with the understanding that the student would receive any *one* hint, not the whole set. The tutors were told that these edits would be communicated to the student without any natural language explanation, so the edits should be interpretable on their own.
2. Each tutor independently reviewed the hints generated by the other two tutors and marked each hint as valid or invalid by the same criteria used to generate hints. We included in our gold standard set any hint which at least two out of three tutors considered to be valid. Our goal was not to determine a definitive, objective set of correct hints but rather to identify a large number of hints that a reasonable human tutor might generate. Requiring that two tutors agreed on each hint provided a higher quality standard than is used in most classrooms, while allowing for differences of opinion among tutors.
3. *For the iSnap datasets only*, the tutors then reconvened to discuss any hints where there was disagreement and resolved these disagreements to produce a final consensus set of gold standard hints.

Each row of the spreadsheet corresponds to one hint authored by the tutors. The CSV has the following columns:

* assignmentID: The ID of the assignment of the hint request.
* requestID: A unique ID for the hint request for which the hint was authored.
* year: The semester when the hint request was made.
* hintID: A year-unique ID for the tutor-authored hint (only guaranteed unique for a given year).
* OneTutor: TRUE if at least one tutor believed this was a valid hint after Phase 2.
* MultipleTutors: TRUE if at least *two* tutors believed this hint was valid after Phase 2.
* Consensus [*Meaningful only for the iSnap dataset*]: TRUE if all tutors agreed that this was a valid hint after Phase 3.
* priority [*Meaningful only for the iSnap dataset*]: For hints where Consensus is TRUE, tutors also came to consensus on the priority of the hint: 1 (highest), 2 (high) and 3 (normal).
* from: A JSON abstract syntax tree for the student's code at the time of the hint request (see the next section for a description of the JSON AST format).
* to: A JSON abstract syntax tree for the student's code after applying the tutor's recommended hint.

## Abstract Syntax Tree JSON Format

All code in these datasets are represented as abstract syntax trees (ASTs), stored in a JSON format. Each JSON object represents a node in the AST, and has the following properties:
* `type` [required]: The type of the node (e.g. "if-statement", "expression", "variable-declaration", etc.). In Snap, this could be the name of a built-in block (e.g. "forward", "turn"). The set of possible types is pre-defined by a given programming language, as they generally correspond to keywords. The possible types for a given language are defined in the grammar file for the dataset, discussed later.
* `value` [optional]: This contains any user-defined value for the node, such as the identifier for a variable or function, the value of a literal, or the name of an imported module These are things the student names, and they could take any value. **Note**: In the Snap datasets, string literal values have been removed to anonymize the dataset; however, these values are generally not relevant for hint generation.
* `children` [optional]: A map of this node's children, if any. In Python, the keys of the map indicate the relationship of the parent/child (e.g. a while loop might have a "condition" child). In the Snap dataset, they are simply numbers indicating the ordering of the children (e.g. arguments "0", "1" and "2"). The values are objects representing the children.
* `children-order` [optional]: The order of this node's children, represented as an array of keys from the `children` map. This is necessary because JSON maps have no ordering, though the order of the children in the map should correspond to the correct order.

## Grammar Files

The snap-grammar.json and python-grammar.json files may be useful in generating hints. They define the valid format for ASTs in the given languages. Note that these files are generated automatically based on the corpus of data in these datasets, not using a priori knowledge of the programming languages themselves. As such, they define constraints for the ASTs that *have been seen* in this dataset, with the assumption that a valid hint will also follow these constraints. The grammars are represented as a JSON object with the following fields:

### root
An array of the permitted `node_types` or `categories` for the root node.

### node_types
A map of valid node types, with the key corresponding to the type itself, and the value being a JSON object with the following fields:
* `type`: Whether this node should have a `fixed` number of children or a `flexible` number of children.
* `count`: If the type is `fixed`, the number of children this node should have.
* `1`...`n`: If the type is `fixed`, each entry is an array of the permitted `node_types` or `categories`  for the `i`th child.
* `permitted_children`: If the type if `flexible`, an array of the permitted `node_types` or `categories`  for the children of this node.

**Note**: Types for children will either be a lowercase type, corresponding to one of the `node_types` *or* one of the uppercase `categories`.

### categories
A map of defined type categories, with keys corresponding to the name of the category and values being a list of types which fall into that category. This makes it more feasible to list the valid child types for a given node, since many nodes can have whole categories of types as children. For example, in Snap, the `REPORTER` category includes all built-in procedures with a return value. **Note** that some types may fall into multiple categories.

### special_types
A list of all types which do not fall into a category.

## Programming Problems

This section contains descriptions of the programming problems included in these datasets.

### iSnap Datasets

For a complete description with pictures, please see [this document](https://docs.google.com/document/d/1YxYPsagMO7CUFxV7J2e3cL1i39Kn6V6oF93UqolxkVI/pub).

#### Squiral (squiralHW)

Students completed this activity for homework and had a week to complete it. The goal is to define a procedure to draw a square-sprial pattern. Common solutions are ~10 lines of code.

**Instructions**: In this activity you will build a block, in SNAP, that makes your sprite draw a squiral like the one below:

![Squairal Image](https://lh3.googleusercontent.com/u_oZCblVlwxxOnDil0jWeA3Nlqlgip3Y3wZdY2-Z-YJAY1-3nEIgnGUkVGUX-bhKMLw8KSptIM3ib52iBIJteFrmoGuxETgYV_X8YPElIbO_GpLcYIADvMo7kC7V-TtfQQlRfxPAAntxUidyUQ)

Give the block an argument that allows the user to set the number of times the sprite completes a full rotation around the center. (The picture has at least 5 rotations).

#### Guessing Game (guess1Lab)

Students completed this activity in a lab with an undergraduate teaching assistants available to help them. The goal is to create a "guessing game" where the computer comes up with a random number and the player must guess it. Common solutions are ~13 lines of code.

**Instructions**:
1. The computer chooses a random number between 1 and 10 and continuously asks the user to guess the number until they guess correctly.
2. Make sure that your program contains the following:
  * Welcome the user to the game
  * Ask the user's name
  * Welcome the user by name
  * Tell the user if their guess was too high, too low, or correct

### ITAP Dataset

#### firstAndLast
Given a string, s, return the combination of the first letter and the last letter of the string. You can assume that s is at least two characters long.

*Example Solution*:
```python
    def firstAndLast(s):
        return s[0] + s[len(s) - 1]
```

####  helloWorld
Write a function, hello_world, which takes no parameters and returns the string "Hello World!".

*Example Solution*:
```python
    def helloWorld():
        return "Hello World!"
```

#### oneToN
Given a number n, return a string that contains the numbers from 1 to n. (So 5 results in "12345")

*Example Solutions*:
```python
    def oneToN(n):
        return "".join([str(i) for i in range(1, n + 1)])

    def oneToN(n):
        text = ""
        for i in range(1, (n + 1)):
	        text += str(i)
        return text
```

#### isPunctuation
Given a one-character string, write a function, is_punctuation, which returns whether that character is a punctuation mark. Hint: you can do this by importing the string module and using the built-in punctuation value.

*Example Solution*:
```python
    import string
    def isPunctuation(c):
        return c in string.punctuation
```

#### kthDigit
Given a number, x, return the kth digit (from the back) of x. In the number 1234, the 1st digit is 4, the 2nd is 3, etc. You can assume that x is positive. Note that // is truncating integer division in python.

*Example Solutions*:
```python
    def kthDigit(x, k):
        return (x // (10 ** (k - 1))) % 10

    def kthDigit(x, k):
        return (x % (10 ** k)) // (10 ** (k - 1))
```

## References

[1] Price, T. W., R. Zhi, Y. Dong, N. Lytle and T. Barnes. "The Impact of Data Quantity and Source on the Quality of Data-driven Hints for Programming." International Conference on Artificial Intelligence in Education. 2018.

[2] Price, T. W., Y. Dong and D. Lipovac. "iSnap: Towards Intelligent Tutoring in Novice Programming Environments." ACM Special Interest Group on Computer Science Education (SIGCSE). 2017.

[3] Rivers, K. and K. R. Koedinger, "Data-Driven Hint Generation in Vast Solution Spaces: a Self-Improving Python Programming Tutor," International Journal of Artificial Intelligence i  Education, vol. 27, no. 1, pp. 37–64, 2017.

[4] Rivers, K., E. Harpstead, and K. Koedinger, "Learning Curve Analysis for Programming: Which Concepts do Students Struggle With?," in Proceedings of the International Computing Education Research Conference, 2016, pp. 143–151.